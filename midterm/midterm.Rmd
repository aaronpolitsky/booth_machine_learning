---
title: "Midterm"
author: "Aaron Politsky"
date: "October 30, 2015"
output: pdf_document
---

#1

Let's source our dependencies and load our libraries.
```{r, message=F}
library(caret)
library(data.table)
library(doParallel)
library(plyr)
source("EvaluationMetrics.R")
```

And set up parallelization:
```{r, message=F}
cl <- makeCluster(detectCores())   # I don't mind using all of my cores
clusterEvalQ(cl, library(foreach))
registerDoParallel(cl)   # register this cluster
```

Now, import our Accident data:
```{r}

set.seed(99) # Gretzky.  Why not?

data_folder_path <- 'data'

accidents <- as.data.table(read.table(
  file.path("data", 'Accidents.csv'),
  header=TRUE, sep=',', stringsAsFactors=T))


```


##1.1 Choosing a Variable to Predict
MAX_SEV_IR indicates the maximum severity of the accident, and has three levels:

- 0: no injury
- 1: non-fatal injury
- 2: fatal injury

Let's assume we are interested in predicting if there is a severe injury that requires medical attention.  One might argue that responders cannot help a dead-on-arrival person, and that fatal injury status should be disregarded.  But since MAX_SEV_IR indicates the maximum severity of the accident, it is possible that even if there is a fatality, we do not know if there are non-fatal, serious injuries that require care.  (Assume that even with a MAX_SEV_IR:2 accident in which we know there was only one person involved, we are still better safe than sorry and should respond as if all is not lost.)  So, let's create another variable called severe.injury which is "no" if MAX_SEV_IR indicates no injury and "yes" if otherwise.  

```{r}
accidents$severe.injury <- accidents$MAX_SEV_IR != 0
accidents[, severe.injury := factor(severe.injury,
                                    levels=c(TRUE,FALSE), labels=c("Yes", "No"))]

table(accidents$severe.injury, accidents$MAX_SEV_IR)

```


##1.2 Developing a predictive model

### Choosing input variables
At the time we decide to respond to the accident, it is likely that we know the following:

- HOUR_I_R:  rush hour or not
- INT_HWY	Interstate?
- MAN_COL_I	 collision, head on, or other?
- PED_ACC_R	 pedestrian/cyclist involved?
- REL_JCT_I_R	accident at intersection/interchange or not?
- REL_RWY_R	 accident on roadway or not?
- LGTCON_I_R	Lighting conditions {day, dark, lighted, etc.}
- SPD_LIM	Speed limit, miles per hour 
- TRAF_CON_R	presence and type of Traffic control device
- TRAF_WAY	two-way traffic, divided hwy, one-way road
- VEH_INVL	Number of vehicles involved
- WEATHER_R	adverse weather presence/type

We may know the following: 

- WRK_ZONE	construction zone
- NO_INJ_I	Number of injuries (may be unclear at time of reporting)
- INJURY_CRASH	1=yes, 0= no (we may not be sure at time of reporting, and this data is ex post)
- PRPTYDMG_CRASH	1=property damage, 2=no property damage (only truly known ex post)
- FATALITIES	1= yes, 0= no 

We likely do not know the following, or only know it for certain ex post: 

- ALCOHOL_I	Alcohol involved = 1, not involved = 2
- ALIGN_I	1 = straight, 2 = curve
- STRATUM_R	1= NASS Crashes Involving At Least One Passenger Vehicle, i.e., A Passenger Car, Sport Utility Vehicle, Pickup Truck Or Van) Towed Due To Damage From The Crash Scene And No Medium Or Heavy Trucks Are Involved.
	0=not
- PROFIL_I_R	1= level, 0=other
- SUR_CON	Surface conditions (1=dry, 2=wet, 3=snow/slush, 4=ice, 5=sand/dirt/oil, 8=other, 9=unknown).  Some of these categories are hyper-local (e.g. sand/dirt/oil)
- MAX_SEV_IR	which we profess to not know by construction of this problem.  

Let's choose the definitely known ones: 
```{r}
predictor.names <- 
  c("HOUR_I_R",
    "INT_HWY",
    "MANCOL_I_R",
    "PED_ACC_R",
    "RELJCT_I_R",
    "REL_RWY_R",
    "LGTCON_I_R",
    "SPD_LIM",
    "TRAF_CON_R",
    "TRAF_WAY",
    "VEH_INVL",
    "WEATHER_R"
) 
```


Before we go any further, let's rename some confusingly-named variables:
```{r}
names(accidents)[names(accidents)=='INJURY_CRASH'] = 'injury.not.fatality'
```

Lets convert to factors:
```{r}
for(colname in names(accidents)) {
  accidents[[colname]] <- as.factor(accidents[[colname]])
}
```

Just to sanity-check, the classes of the variables are:

```{r}
sapply(accidents, class)

```

```{r}
num.samples <- nrow(accidents)

```

Out of the **`r formatC(num.samples, format='d', big.mark=',')`** samples, the incidence of a severe injury accident is **`r formatC(100 * sum(accidents$severe.injury == 'Yes') / num.samples, format='f', digits=2, big.mark=',')`%**. Note that this creates a "**skewed classes**" problem: one of the classes of cases (here the "responsive" class) is significantly rarer than the other.

_(**note**: in more extreme cases where one class is much, much rarer than the other to the order of 1000 or 10,000 times, our model fitting procedures would need to be tweaked; but this case is not so extreme)_

We don't have a missing data problem with this data set:

```{r}
sapply(accidents, function(col) sum(is.na(col)))
```

Let's split a Validation set out from the Training data, for use in estimating OOS performance:

```{r}
valid_proportion <- 1 / 3
valid_indices <- createDataPartition(
  y=accidents$severe.injury,
  p=valid_proportion,
  list=FALSE)

accidents.valid <- accidents[valid_indices, ]
accidents <- accidents[-valid_indices, ]
```

Just to sanity-check that the data sets have been split representatively by **`caret`**: the responsive incidences in the Training and Validation sets are **`r formatC(100 * sum(accidents$severe.injury == 'responsive') / nrow(accidents), format='f', digits=2, big.mark=',')`** and **`r formatC(100 * sum(accidents.valid$severe.injury == 'responsive') / nrow(accidents.valid), format='f', digits=2, big.mark=',')`**, respectively.


# Classification Models

Let's train 3 types of classification models: a Random Forest, a Boosted Trees model and a Logistic Regression.

```{r}
caret_optimized_metric <- 'logLoss'   # equivalent to 1 / 2 of Deviance

caret_train_control <- trainControl(
  classProbs=TRUE,             # compute class probabilities
  summaryFunction=mnLogLoss,   # equivalent to 1 / 2 of Deviance
  method='repeatedcv',         # repeated Cross Validation
  number=5,                    # 5 folds
  repeats=6,                   # 3 repeats
  allowParallel=TRUE)
```

```{r message=FALSE, warning=FALSE}
B <- 40#1200

system.time(
  rf_model <- train(
    x=accidents[, predictor.names, with=FALSE],
    y=accidents$severe.injury,
    method='parRF',     # parallel Random Forest
    metric=caret_optimized_metric,
    ntree=B,            # number of trees in the Random Forest
    #nodesize=30,        # minimum node size set small enough to allow for complex trees,
    nodesize=200,        # minimum node size set small enough to allow for complex trees,
   # but not so small as to require too large B to eliminate high variance
    importance=TRUE,    # evaluate importance of predictors
    keep.inbag=TRUE,
    trControl=caret_train_control,
    tuneGrid=NULL)
)

```

```{r}
#B <- 2400
B <- 40

system.time(
  boost_model <- train(
    x=accidents[, predictor.names, with=FALSE],
    y=accidents$severe.injury,
    method='gbm',       # Generalized Boosted Models
    metric=caret_optimized_metric,
    verbose=FALSE,
    trControl=caret_train_control,
    tuneGrid=expand.grid(
      n.trees=B,              # number of trees
      interaction.depth=10,   # max tree depth,
      n.minobsinnode=100,     # minimum node size
      shrinkage=0.01))        # shrinkage parameter, a.k.a. "learning rate"
)
```

```{r}
system.time(
  log_reg_model <- train(
    x=accidents[, predictor.names[3:5], with=FALSE],
    y=accidents$severe.injury,
    preProcess=c('center', 'scale'), 
    method='plr',    # Penalized Logistic Regression
    metric=caret_optimized_metric,
    trControl=caret_train_control,
    tuneGrid=expand.grid(
      lambda=0,      # weight penalty parameter
      cp='aic'
    )
  )
)
```

## Justification Based on Out of Sample Performance
We'll now evaluate the OOS performances of these 3 models on the Validation set to select a model we think is best:

```{r}
low_prob <- 1e-6
high_prob <- 1 - low_prob
log_low_prob <- log(low_prob)
log_high_prob <- log(high_prob)
log_prob_thresholds <- seq(from=log_low_prob, to=log_high_prob, length.out=100)
prob_thresholds <- exp(log_prob_thresholds)

rf_pred_probs <- predict(
  rf_model, newdata=accidents.valid[ , predictor.names, with=FALSE], type='prob')
rf_oos_performance <- bin_classif_eval(
  rf_pred_probs$responsive, accidents.valid$severe.injury, thresholds=prob_thresholds)

boost_pred_probs <- predict(
  boost_model, newdata=accidents.valid[ , predictor.names, with=FALSE], type='prob')
boost_oos_performance <- bin_classif_eval(
  boost_pred_probs$responsive, accidents.valid$severe.injury, thresholds=prob_thresholds)

log_reg_pred_probs <- predict(
  log_reg_model, newdata=accidents.valid[, predictor.names, with=FALSE], type='prob')
log_reg_oos_performance <- bin_classif_eval(
  log_reg_pred_probs$responsive, accidents.valid$severe.injury, thresholds=prob_thresholds)


plot(x=1 - rf_oos_performance$specificity,
     y=rf_oos_performance$sensitivity,
     type = "l", col='darkgreen', lwd=3,
     xlim = c(0., 1.), ylim = c(0., 1.),
     main = "ROC Curves (Validation Data)",
     xlab = "1 - Specificity", ylab = "Sensitivity")
abline(a=0,b=1,lty=2,col=8)
lines(x=1 - boost_oos_performance$specificity,
      y=boost_oos_performance$sensitivity,
      col='green', lwd=3)
lines(x=1 - log_reg_oos_performance$specificity,
      y=log_reg_oos_performance$sensitivity,
      col='red', lwd=3)
legend('right', c('Random Forest', 'Boosted Trees', 'Logistic Regression'), 
   lty=1, col=c('darkgreen', 'green', 'red'), lwd=3, cex=1.)
```
